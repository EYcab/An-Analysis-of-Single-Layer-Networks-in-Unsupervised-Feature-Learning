{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3, 32, 32)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3, 32, 32)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3, 32, 32)\n",
      "Test labels shape:  (1000,)\n",
      "Extracting patch 0 / 400000\n",
      "Extracting patch 10000 / 400000\n",
      "Extracting patch 20000 / 400000\n",
      "Extracting patch 30000 / 400000\n",
      "Extracting patch 40000 / 400000\n",
      "Extracting patch 50000 / 400000\n",
      "Extracting patch 60000 / 400000\n",
      "Extracting patch 70000 / 400000\n",
      "Extracting patch 80000 / 400000\n",
      "Extracting patch 90000 / 400000\n",
      "Extracting patch 100000 / 400000\n",
      "Extracting patch 110000 / 400000\n",
      "Extracting patch 120000 / 400000\n",
      "Extracting patch 130000 / 400000\n",
      "Extracting patch 140000 / 400000\n",
      "Extracting patch 150000 / 400000\n",
      "Extracting patch 160000 / 400000\n",
      "Extracting patch 170000 / 400000\n",
      "Extracting patch 180000 / 400000\n",
      "Extracting patch 190000 / 400000\n",
      "Extracting patch 200000 / 400000\n",
      "Extracting patch 210000 / 400000\n",
      "Extracting patch 220000 / 400000\n",
      "Extracting patch 230000 / 400000\n",
      "Extracting patch 240000 / 400000\n",
      "Extracting patch 250000 / 400000\n",
      "Extracting patch 260000 / 400000\n",
      "Extracting patch 270000 / 400000\n",
      "Extracting patch 280000 / 400000\n",
      "Extracting patch 290000 / 400000\n",
      "Extracting patch 300000 / 400000\n",
      "Extracting patch 310000 / 400000\n",
      "Extracting patch 320000 / 400000\n",
      "Extracting patch 330000 / 400000\n",
      "Extracting patch 340000 / 400000\n",
      "Extracting patch 350000 / 400000\n",
      "Extracting patch 360000 / 400000\n",
      "Extracting patch 370000 / 400000\n",
      "Extracting patch 380000 / 400000\n",
      "Extracting patch 390000 / 400000\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "from neural_net import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    # easyier for py\n",
    "    X_train=X_train.swapaxes(1,3)\n",
    "    X_val=X_val.swapaxes(1,3)\n",
    "    X_test=X_test.swapaxes(1,3)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "\n",
    "rfSize = 6\n",
    "numCentroids=1600\n",
    "whitening=True\n",
    "numPatches = 400000\n",
    "CIFAR_DIM=[32,32,3]\n",
    "\n",
    "#create unsurpervised data\n",
    "patches=[]\n",
    "for i in range(numPatches):\n",
    "    if(np.mod(i,10000) == 0):\n",
    "        print \"sampling for Kmeans\",i,\"/\",numPatches\n",
    "    start_r=np.random.randint(CIFAR_DIM[0]-rfSize)\n",
    "    start_c=np.random.randint(CIFAR_DIM[1]-rfSize)\n",
    "    patch=np.array([])\n",
    "    img=X_train[np.mod(i,X_train.shape[0])]\n",
    "    for layer in img:\n",
    "        patch=np.append(patch,layer[start_r:start_r+rfSize].T[start_c:start_c+rfSize].T.ravel())\n",
    "    patches.append(patch)\n",
    "patches=np.array(patches)\n",
    "#normalize patches\n",
    "patches=(patches-patches.mean(1)[:,None])/np.sqrt(patches.var(1)+10)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whitening\n",
    "\n",
    "[D,V]=np.linalg.eig(np.cov(patches,rowvar=0))\n",
    "\n",
    "P = V.dot(np.diag(np.sqrt(1/(D + 0.1)))).dot(V.T)\n",
    "patches = patches.dot(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids=np.random.randn(numCentroids,patches.shape[1])*.1\n",
    "num_iters=50\n",
    "batch_size=1000#CSIL do not have enough memory, dam\n",
    "for ite in range(num_iters):\n",
    "    print \"kmeans iters\",ite+1,\"/\",num_iters\n",
    "#     c2=.5*np.power(centroids,2).sum(1)\n",
    "#     idx=np.argmax(patches.dot(centroids.T)-c2,axis=1) # x2 the same omit\n",
    "    hf_c2_sum=.5*np.power(centroids,2).sum(1)\n",
    "    counts=np.zeros(numCentroids)\n",
    "    summation=np.zeros_like(centroids)\n",
    "    for i in range(0,len(patches),batch_size):\n",
    "        last_i=min(i+batch_size,len(patches))\n",
    "        idx=np.argmax(patches[i:last_i].dot(centroids.T)\\\n",
    "                  -hf_c2_sum.T,\\\n",
    "                  axis=1)        \n",
    "        S=np.zeros([last_i-i,numCentroids])\n",
    "        S[range(last_i-i),\n",
    "          np.argmax(patches[i:last_i].dot(centroids.T)-hf_c2_sum.T\n",
    "                    ,axis=1)]=1\n",
    "        summation+=S.T.dot(patches[i:last_i])\n",
    "        counts+=S.sum(0)\n",
    "    centroids=summation/counts[:,None]\n",
    "    centroids[counts==0]=0 # some centroids didn't get members, divide by zero\n",
    "    #the thing is, they will stay zero forever\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding(img,window=[6,6]):\n",
    "    out=np.array([])\n",
    "    for i in range(3):\n",
    "        s=img.shape\n",
    "        row=s[1]\n",
    "        col=s[2]\n",
    "        col_extent = col - window[1] + 1\n",
    "        row_extent = row - window[0] + 1\n",
    "        start_idx = np.arange(window[0])[:,None]*col + np.arange(window[1])\n",
    "        offset_idx = np.arange(row_extent)[:,None]*col + np.arange(col_extent)\n",
    "        if len(out)==0:\n",
    "            out=np.take (img[i],start_idx.ravel()[:,None] + offset_idx.ravel())\n",
    "        else:\n",
    "            out=np.append(out,np.take (img[i],start_idx.ravel()[:,None] + offset_idx.ravel()),axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract feature 1000 / 49000\n",
      "extract feature 2000 / 49000\n",
      "extract feature 3000 / 49000\n",
      "extract feature 4000 / 49000\n",
      "extract feature 5000 / 49000\n",
      "extract feature 6000 / 49000\n",
      "extract feature 7000 / 49000\n",
      "extract feature 8000 / 49000\n"
     ]
    }
   ],
   "source": [
    "trainXC=[]\n",
    "idx=0\n",
    "for img in X_train:\n",
    "    idx+=1\n",
    "    if not np.mod(idx,1000):\n",
    "        print \"extract features\",idx,'/',len(X_train)\n",
    "    patches=sliding(img,[rfSize,rfSize]).T\n",
    "    #normalize\n",
    "    patches=(patches-patches.mean(1)[:,None])/(np.sqrt(patches.var(1)+10)[:,None])\n",
    "    #map to feature space\n",
    "    patches=patches.dot(P)\n",
    "    #calculate distance using x2-2xc+c2\n",
    "    x2=np.power(patches,2).sum(1)\n",
    "    c2=np.power(centroids,2).sum(1)\n",
    "    xc=patches.dot(centroids.T)\n",
    "\n",
    "    dist=np.sqrt(-2*xc+x2[:,None]+c2)\n",
    "    u=dist.mean(1)\n",
    "    patches=np.maximum(-dist+u[:,None],0)\n",
    "    rs=CIFAR_DIM[0]-rfSize+1\n",
    "    cs=CIFAR_DIM[1]-rfSize+1\n",
    "    patches=np.reshape(patches,[rs,cs,-1])\n",
    "    q=[]\n",
    "    q.append(patches[0:rs/2,0:cs/2].sum(0).sum(0))\n",
    "    q.append(patches[0:rs/2,cs/2:cs-1].sum(0).sum(0))\n",
    "    q.append(patches[rs/2:rs-1,0:cs/2].sum(0).sum(0))\n",
    "    q.append(patches[rs/2:rs-1,cs/2:cs-1].sum(0).sum(0))\n",
    "    q=np.array(q).ravel()\n",
    "    trainXC.append(q)\n",
    "trainXC=np.array(trainXC)\n",
    "trainXC=(trainXC-trainXC.mean(1)[:,None])/(np.sqrt(trainXC.var(1)+.01)[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neural_net import *\n",
    "\n",
    "input_size = trainXC.shape[1]\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes,1e-5)\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                            num_iters=10000, batch_size=100,\n",
    "                            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "                            reg=0, verbose=True,update=\"momentum\",arg=0.9,dropout=0.5)\n",
    "val_acc = (net.predict(X_train) == y_train).mean()\n",
    "print 'Train accuracy: ', val_acc\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print 'Validation accuracy: ', val_acc\n",
    "val_acc = (net.predict(X_test) == y_test).mean()\n",
    "print 'Test accuracy: ', val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the loss function and train / validation accuracies\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(\"dropout loss_history.eps\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.savefig('dropout accuracy.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
