{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3, 32, 32)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3, 32, 32)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3, 32, 32)\n",
      "Test labels shape:  (1000,)\n",
      "Extracting patch 0 / 400000\n",
      "Extracting patch 10000 / 400000\n",
      "Extracting patch 20000 / 400000\n",
      "Extracting patch 30000 / 400000\n",
      "Extracting patch 40000 / 400000\n",
      "Extracting patch 50000 / 400000\n",
      "Extracting patch 60000 / 400000\n",
      "Extracting patch 70000 / 400000\n",
      "Extracting patch 80000 / 400000\n",
      "Extracting patch 90000 / 400000\n",
      "Extracting patch 100000 / 400000\n",
      "Extracting patch 110000 / 400000\n",
      "Extracting patch 120000 / 400000\n",
      "Extracting patch 130000 / 400000\n",
      "Extracting patch 140000 / 400000\n",
      "Extracting patch 150000 / 400000\n",
      "Extracting patch 160000 / 400000\n",
      "Extracting patch 170000 / 400000\n",
      "Extracting patch 180000 / 400000\n",
      "Extracting patch 190000 / 400000\n",
      "Extracting patch 200000 / 400000\n",
      "Extracting patch 210000 / 400000\n",
      "Extracting patch 220000 / 400000\n",
      "Extracting patch 230000 / 400000\n",
      "Extracting patch 240000 / 400000\n",
      "Extracting patch 250000 / 400000\n",
      "Extracting patch 260000 / 400000\n",
      "Extracting patch 270000 / 400000\n",
      "Extracting patch 280000 / 400000\n",
      "Extracting patch 290000 / 400000\n",
      "Extracting patch 300000 / 400000\n",
      "Extracting patch 310000 / 400000\n",
      "Extracting patch 320000 / 400000\n",
      "Extracting patch 330000 / 400000\n",
      "Extracting patch 340000 / 400000\n",
      "Extracting patch 350000 / 400000\n",
      "Extracting patch 360000 / 400000\n",
      "Extracting patch 370000 / 400000\n",
      "Extracting patch 380000 / 400000\n",
      "Extracting patch 390000 / 400000\n"
     ]
    }
   ],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "from neural_net import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    # easyier for py\n",
    "    X_train=X_train.swapaxes(1,3)\n",
    "    X_val=X_val.swapaxes(1,3)\n",
    "    X_test=X_test.swapaxes(1,3)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "\n",
    "rfSize = 6\n",
    "numCentroids=1600\n",
    "whitening=True\n",
    "numPatches = 400000\n",
    "CIFAR_DIM=[32,32,3]\n",
    "\n",
    "#create unsurpervised data\n",
    "patches=[]\n",
    "for i in range(numPatches):\n",
    "    if(np.mod(i,10000) == 0):\n",
    "        print \"sampling for Kmeans\",i,\"/\",numPatches\n",
    "    start_r=np.random.randint(CIFAR_DIM[0]-rfSize)\n",
    "    start_c=np.random.randint(CIFAR_DIM[1]-rfSize)\n",
    "    patch=np.array([])\n",
    "    img=X_train[np.mod(i,X_train.shape[0])]\n",
    "    for layer in img:\n",
    "        patch=np.append(patch,layer[start_r:start_r+rfSize].T[start_c:start_c+rfSize].T.ravel())\n",
    "    patches.append(patch)\n",
    "patches=np.array(patches)\n",
    "#normalize patches\n",
    "patches=(patches-patches.mean(1)[:,None])/np.sqrt(patches.var(1)+10)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whitening\n",
    "\n",
    "[D,V]=np.linalg.eig(np.cov(patches,rowvar=0))\n",
    "\n",
    "P = V.dot(np.diag(np.sqrt(1/(D + 0.1)))).dot(V.T)\n",
    "patches = patches.dot(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids=np.random.randn(numCentroids,patches.shape[1])*.1\n",
    "num_iters=50\n",
    "batch_size=1000#CSIL do not have enough memory, dam\n",
    "for ite in range(num_iters):\n",
    "    print \"kmeans iters\",ite+1,\"/\",num_iters\n",
    "#     c2=.5*np.power(centroids,2).sum(1)\n",
    "#     idx=np.argmax(patches.dot(centroids.T)-c2,axis=1) # x2 the same omit\n",
    "    hf_c2_sum=.5*np.power(centroids,2).sum(1)\n",
    "    counts=np.zeros(numCentroids)\n",
    "    summation=np.zeros_like(centroids)\n",
    "    for i in range(0,len(patches),batch_size):\n",
    "        last_i=min(i+batch_size,len(patches))\n",
    "        idx=np.argmax(patches[i:last_i].dot(centroids.T)\\\n",
    "                  -hf_c2_sum.T,\\\n",
    "                  axis=1)        \n",
    "        S=np.zeros([last_i-i,numCentroids])\n",
    "        S[range(last_i-i),\n",
    "          np.argmax(patches[i:last_i].dot(centroids.T)-hf_c2_sum.T\n",
    "                    ,axis=1)]=1\n",
    "        summation+=S.T.dot(patches[i:last_i])\n",
    "        counts+=S.sum(0)\n",
    "    centroids=summation/counts[:,None]\n",
    "    centroids[counts==0]=0 # some centroids didn't get members, divide by zero\n",
    "    #the thing is, they will stay zero forever\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding(img,window=[6,6]):\n",
    "    out=np.array([])\n",
    "    for i in range(3):\n",
    "        s=img.shape\n",
    "        row=s[1]\n",
    "        col=s[2]\n",
    "        col_extent = col - window[1] + 1\n",
    "        row_extent = row - window[0] + 1\n",
    "        start_idx = np.arange(window[0])[:,None]*col + np.arange(window[1])\n",
    "        offset_idx = np.arange(row_extent)[:,None]*col + np.arange(col_extent)\n",
    "        if len(out)==0:\n",
    "            out=np.take (img[i],start_idx.ravel()[:,None] + offset_idx.ravel())\n",
    "        else:\n",
    "            out=np.append(out,np.take (img[i],start_idx.ravel()[:,None] + offset_idx.ravel()),axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(X_train):\n",
    "    trainXC=[]\n",
    "    idx=0\n",
    "    for img in X_train:\n",
    "        idx+=1\n",
    "        if not np.mod(idx,1000):\n",
    "            print \"extract features\",idx,'/',len(X_train)\n",
    "        patches=sliding(img,[rfSize,rfSize]).T\n",
    "        #normalize\n",
    "        patches=(patches-patches.mean(1)[:,None])/(np.sqrt(patches.var(1)+10)[:,None])\n",
    "        #map to feature space\n",
    "        patches=patches.dot(P)\n",
    "        #calculate distance using x2-2xc+c2\n",
    "        x2=np.power(patches,2).sum(1)\n",
    "        c2=np.power(centroids,2).sum(1)\n",
    "        xc=patches.dot(centroids.T)\n",
    "\n",
    "        dist=np.sqrt(-2*xc+x2[:,None]+c2)\n",
    "        u=dist.mean(1)\n",
    "        patches=np.maximum(-dist+u[:,None],0)\n",
    "        rs=CIFAR_DIM[0]-rfSize+1\n",
    "        cs=CIFAR_DIM[1]-rfSize+1\n",
    "        patches=np.reshape(patches,[rs,cs,-1])\n",
    "        q=[]\n",
    "        q.append(patches[0:rs/2,0:cs/2].sum(0).sum(0))\n",
    "        q.append(patches[0:rs/2,cs/2:cs-1].sum(0).sum(0))\n",
    "        q.append(patches[rs/2:rs-1,0:cs/2].sum(0).sum(0))\n",
    "        q.append(patches[rs/2:rs-1,cs/2:cs-1].sum(0).sum(0))\n",
    "        q=np.array(q).ravel()\n",
    "        trainXC.append(q)\n",
    "    trainXC=np.array(trainXC)\n",
    "    trainXC=(trainXC-trainXC.mean(1)[:,None])/(np.sqrt(trainXC.var(1)+.01)[:,None])\n",
    "    return trainXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract features 1000 / 1000\n"
     ]
    }
   ],
   "source": [
    "valXC=extract_features(X_val)\n",
    "\n",
    "testXC=extract_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open(\"features.pickle\",\"w\") as f:\n",
    "    pickle.dump([trainXC,valXC,testXC,y_train,y_val,y_test],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 20000: loss 2.301914\n",
      "iteration 200 / 20000: loss 2.279830\n",
      "iteration 300 / 20000: loss 1.978543\n",
      "iteration 400 / 20000: loss 2.001408\n",
      "train_acc 0.320000, val_acc 0.289000, time 0\n",
      "iteration 500 / 20000: loss 1.821423\n",
      "iteration 600 / 20000: loss 1.891150\n",
      "iteration 700 / 20000: loss 1.809079\n",
      "iteration 800 / 20000: loss 1.902541\n",
      "iteration 900 / 20000: loss 1.843838\n",
      "train_acc 0.360000, val_acc 0.413000, time 0\n",
      "iteration 1000 / 20000: loss 1.774987\n",
      "iteration 1100 / 20000: loss 1.561544\n",
      "iteration 1200 / 20000: loss 1.503797\n",
      "iteration 1300 / 20000: loss 1.542494\n",
      "iteration 1400 / 20000: loss 1.585046\n",
      "train_acc 0.460000, val_acc 0.494000, time 0\n",
      "iteration 1500 / 20000: loss 1.524860\n",
      "iteration 1600 / 20000: loss 1.341965\n",
      "iteration 1700 / 20000: loss 1.364933\n",
      "iteration 1800 / 20000: loss 1.474574\n",
      "iteration 1900 / 20000: loss 1.401914\n",
      "train_acc 0.550000, val_acc 0.528000, time 1\n",
      "iteration 2000 / 20000: loss 1.426933\n",
      "iteration 2100 / 20000: loss 1.424983\n",
      "iteration 2200 / 20000: loss 1.308591\n",
      "iteration 2300 / 20000: loss 1.213415\n",
      "iteration 2400 / 20000: loss 1.315120\n",
      "train_acc 0.550000, val_acc 0.545000, time 1\n",
      "iteration 2500 / 20000: loss 1.501411\n",
      "iteration 2600 / 20000: loss 1.305750\n",
      "iteration 2700 / 20000: loss 1.459551\n",
      "iteration 2800 / 20000: loss 1.303908\n",
      "iteration 2900 / 20000: loss 1.250637\n",
      "train_acc 0.570000, val_acc 0.548000, time 1\n",
      "iteration 3000 / 20000: loss 1.272815\n",
      "iteration 3100 / 20000: loss 1.012907\n",
      "iteration 3200 / 20000: loss 1.166144\n",
      "iteration 3300 / 20000: loss 1.359543\n",
      "iteration 3400 / 20000: loss 1.108953\n",
      "train_acc 0.630000, val_acc 0.575000, time 1\n",
      "iteration 3500 / 20000: loss 1.265845\n",
      "iteration 3600 / 20000: loss 1.270117\n",
      "iteration 3700 / 20000: loss 1.143037\n",
      "iteration 3800 / 20000: loss 1.190354\n",
      "iteration 3900 / 20000: loss 1.105137\n",
      "train_acc 0.630000, val_acc 0.575000, time 2\n",
      "iteration 4000 / 20000: loss 1.338600\n",
      "iteration 4100 / 20000: loss 1.023489\n",
      "iteration 4200 / 20000: loss 1.195864\n",
      "iteration 4300 / 20000: loss 1.098579\n",
      "iteration 4400 / 20000: loss 1.278054\n",
      "train_acc 0.640000, val_acc 0.587000, time 2\n",
      "iteration 4500 / 20000: loss 1.139674\n",
      "iteration 4600 / 20000: loss 1.110793\n",
      "iteration 4700 / 20000: loss 1.250672\n",
      "iteration 4800 / 20000: loss 1.266390\n",
      "iteration 4900 / 20000: loss 1.344040\n",
      "train_acc 0.590000, val_acc 0.583000, time 2\n",
      "iteration 5000 / 20000: loss 1.148387\n",
      "iteration 5100 / 20000: loss 1.285969\n",
      "iteration 5200 / 20000: loss 1.087090\n",
      "iteration 5300 / 20000: loss 1.160684\n",
      "train_acc 0.670000, val_acc 0.606000, time 2\n",
      "iteration 5400 / 20000: loss 1.318543\n",
      "iteration 5500 / 20000: loss 0.990566\n",
      "iteration 5600 / 20000: loss 1.281428\n",
      "iteration 5700 / 20000: loss 1.248681\n",
      "iteration 5800 / 20000: loss 1.149574\n",
      "train_acc 0.620000, val_acc 0.602000, time 3\n",
      "iteration 5900 / 20000: loss 1.245236\n",
      "iteration 6000 / 20000: loss 1.242080\n",
      "iteration 6100 / 20000: loss 1.080681\n",
      "iteration 6200 / 20000: loss 1.075004\n",
      "iteration 6300 / 20000: loss 1.085706\n",
      "train_acc 0.670000, val_acc 0.612000, time 3\n",
      "iteration 6400 / 20000: loss 0.942279\n",
      "iteration 6500 / 20000: loss 1.227358\n",
      "iteration 6600 / 20000: loss 1.037950\n",
      "iteration 6700 / 20000: loss 1.130149\n",
      "iteration 6800 / 20000: loss 1.029725\n",
      "train_acc 0.650000, val_acc 0.618000, time 3\n",
      "iteration 6900 / 20000: loss 1.093384\n",
      "iteration 7000 / 20000: loss 1.197994\n",
      "iteration 7100 / 20000: loss 1.206386\n",
      "iteration 7200 / 20000: loss 1.085265\n",
      "iteration 7300 / 20000: loss 1.084057\n",
      "train_acc 0.620000, val_acc 0.605000, time 4\n",
      "iteration 7400 / 20000: loss 1.055695\n",
      "iteration 7500 / 20000: loss 1.070150\n",
      "iteration 7600 / 20000: loss 1.083712\n",
      "iteration 7700 / 20000: loss 1.245910\n",
      "iteration 7800 / 20000: loss 1.024917\n",
      "train_acc 0.610000, val_acc 0.625000, time 4\n",
      "iteration 7900 / 20000: loss 1.097237\n",
      "iteration 8000 / 20000: loss 1.267764\n",
      "iteration 8100 / 20000: loss 1.210694\n",
      "iteration 8200 / 20000: loss 1.120250\n",
      "iteration 8300 / 20000: loss 1.065422\n",
      "train_acc 0.650000, val_acc 0.631000, time 4\n",
      "iteration 8400 / 20000: loss 1.016677\n",
      "iteration 8500 / 20000: loss 1.149391\n",
      "iteration 8600 / 20000: loss 1.113846\n",
      "iteration 8700 / 20000: loss 1.005385\n",
      "iteration 8800 / 20000: loss 1.255027\n",
      "train_acc 0.710000, val_acc 0.624000, time 4\n",
      "iteration 8900 / 20000: loss 1.000974\n",
      "iteration 9000 / 20000: loss 1.280439\n",
      "iteration 9100 / 20000: loss 1.139267\n",
      "iteration 9200 / 20000: loss 0.995258\n",
      "iteration 9300 / 20000: loss 0.940049\n",
      "train_acc 0.690000, val_acc 0.620000, time 5\n",
      "iteration 9400 / 20000: loss 1.205098\n",
      "iteration 9500 / 20000: loss 1.137690\n",
      "iteration 9600 / 20000: loss 1.151756\n",
      "iteration 9700 / 20000: loss 1.110586\n",
      "iteration 9800 / 20000: loss 1.178744\n",
      "train_acc 0.580000, val_acc 0.632000, time 5\n",
      "iteration 9900 / 20000: loss 1.048454\n",
      "iteration 10000 / 20000: loss 0.930036\n",
      "iteration 10100 / 20000: loss 1.017364\n",
      "iteration 10200 / 20000: loss 1.090795\n",
      "train_acc 0.670000, val_acc 0.634000, time 5\n",
      "iteration 10300 / 20000: loss 1.036754\n",
      "iteration 10400 / 20000: loss 1.127924\n",
      "iteration 10500 / 20000: loss 1.039577\n",
      "iteration 10600 / 20000: loss 1.029824\n",
      "iteration 10700 / 20000: loss 1.116328\n",
      "train_acc 0.680000, val_acc 0.630000, time 5\n",
      "iteration 10800 / 20000: loss 0.939278\n",
      "iteration 10900 / 20000: loss 1.011187\n",
      "iteration 11000 / 20000: loss 1.135666\n",
      "iteration 11100 / 20000: loss 0.969476\n",
      "iteration 11200 / 20000: loss 1.269489\n",
      "train_acc 0.700000, val_acc 0.625000, time 6\n",
      "iteration 11300 / 20000: loss 1.057334\n",
      "iteration 11400 / 20000: loss 1.033809\n",
      "iteration 11500 / 20000: loss 1.057196\n",
      "iteration 11600 / 20000: loss 1.346611\n",
      "iteration 11700 / 20000: loss 1.164069\n",
      "train_acc 0.630000, val_acc 0.638000, time 6\n",
      "iteration 11800 / 20000: loss 1.241816\n",
      "iteration 11900 / 20000: loss 0.898073\n",
      "iteration 12000 / 20000: loss 0.885001\n",
      "iteration 12100 / 20000: loss 0.872065\n",
      "iteration 12200 / 20000: loss 1.041537\n",
      "train_acc 0.660000, val_acc 0.644000, time 6\n",
      "iteration 12300 / 20000: loss 1.132920\n",
      "iteration 12400 / 20000: loss 1.131360\n",
      "iteration 12500 / 20000: loss 0.970034\n",
      "iteration 12600 / 20000: loss 0.998164\n",
      "iteration 12700 / 20000: loss 1.054048\n",
      "train_acc 0.700000, val_acc 0.648000, time 6\n",
      "iteration 12800 / 20000: loss 0.944543\n",
      "iteration 12900 / 20000: loss 1.075593\n",
      "iteration 13000 / 20000: loss 0.971948\n",
      "iteration 13100 / 20000: loss 0.951291\n",
      "iteration 13200 / 20000: loss 0.986523\n",
      "train_acc 0.620000, val_acc 0.646000, time 7\n",
      "iteration 13300 / 20000: loss 0.866578\n",
      "iteration 13400 / 20000: loss 1.049909\n",
      "iteration 13500 / 20000: loss 1.190125\n",
      "iteration 13600 / 20000: loss 0.910397\n",
      "iteration 13700 / 20000: loss 1.010158\n",
      "train_acc 0.750000, val_acc 0.647000, time 7\n",
      "iteration 13800 / 20000: loss 1.093387\n",
      "iteration 13900 / 20000: loss 0.930451\n",
      "iteration 14000 / 20000: loss 1.037497\n",
      "iteration 14100 / 20000: loss 0.993562\n",
      "iteration 14200 / 20000: loss 1.002881\n",
      "train_acc 0.640000, val_acc 0.647000, time 7\n",
      "iteration 14300 / 20000: loss 0.925038\n",
      "iteration 14400 / 20000: loss 0.910546\n",
      "iteration 14500 / 20000: loss 0.836614\n",
      "iteration 14600 / 20000: loss 1.024461\n",
      "iteration 14700 / 20000: loss 1.246325\n",
      "train_acc 0.600000, val_acc 0.645000, time 7\n",
      "iteration 14800 / 20000: loss 1.033734\n",
      "iteration 14900 / 20000: loss 1.038428\n",
      "iteration 15000 / 20000: loss 0.997000\n",
      "iteration 15100 / 20000: loss 0.962266\n",
      "train_acc 0.700000, val_acc 0.652000, time 8\n",
      "iteration 15200 / 20000: loss 1.226465\n",
      "iteration 15300 / 20000: loss 0.933073\n",
      "iteration 15400 / 20000: loss 0.820595\n",
      "iteration 15500 / 20000: loss 1.013888\n",
      "iteration 15600 / 20000: loss 0.919958\n",
      "train_acc 0.670000, val_acc 0.644000, time 8\n",
      "iteration 15700 / 20000: loss 1.012135\n",
      "iteration 15800 / 20000: loss 1.046003\n",
      "iteration 15900 / 20000: loss 0.981904\n",
      "iteration 16000 / 20000: loss 0.955175\n",
      "iteration 16100 / 20000: loss 1.256643\n",
      "train_acc 0.630000, val_acc 0.648000, time 8\n",
      "iteration 16200 / 20000: loss 0.929180\n",
      "iteration 16300 / 20000: loss 0.860146\n",
      "iteration 16400 / 20000: loss 1.047431\n",
      "iteration 16500 / 20000: loss 1.069678\n",
      "iteration 16600 / 20000: loss 1.332984\n",
      "train_acc 0.710000, val_acc 0.655000, time 8\n",
      "iteration 16700 / 20000: loss 1.140793\n",
      "iteration 16800 / 20000: loss 1.064400\n",
      "iteration 16900 / 20000: loss 1.110933\n",
      "iteration 17000 / 20000: loss 0.941795\n",
      "iteration 17100 / 20000: loss 0.856794\n",
      "train_acc 0.570000, val_acc 0.658000, time 9\n",
      "iteration 17200 / 20000: loss 0.967749\n",
      "iteration 17300 / 20000: loss 1.144966\n",
      "iteration 17400 / 20000: loss 1.048825\n",
      "iteration 17500 / 20000: loss 0.928852\n",
      "iteration 17600 / 20000: loss 0.933018\n",
      "train_acc 0.620000, val_acc 0.655000, time 9\n",
      "iteration 17700 / 20000: loss 0.922048\n",
      "iteration 17800 / 20000: loss 1.017380\n",
      "iteration 17900 / 20000: loss 1.149262\n",
      "iteration 18000 / 20000: loss 1.145672\n",
      "iteration 18100 / 20000: loss 1.013293\n",
      "train_acc 0.740000, val_acc 0.660000, time 9\n",
      "iteration 18200 / 20000: loss 0.902836\n",
      "iteration 18300 / 20000: loss 1.018529\n",
      "iteration 18400 / 20000: loss 1.086048\n",
      "iteration 18500 / 20000: loss 0.829145\n",
      "iteration 18600 / 20000: loss 0.885212\n",
      "train_acc 0.670000, val_acc 0.658000, time 9\n",
      "iteration 18700 / 20000: loss 1.020478\n",
      "iteration 18800 / 20000: loss 0.956787\n",
      "iteration 18900 / 20000: loss 1.065422\n",
      "iteration 19000 / 20000: loss 0.914574\n",
      "iteration 19100 / 20000: loss 0.904167\n",
      "train_acc 0.750000, val_acc 0.664000, time 10\n",
      "iteration 19200 / 20000: loss 0.882998\n",
      "iteration 19300 / 20000: loss 1.013802\n",
      "iteration 19400 / 20000: loss 0.927474\n",
      "iteration 19500 / 20000: loss 0.985463\n",
      "iteration 19600 / 20000: loss 1.012495\n",
      "train_acc 0.630000, val_acc 0.664000, time 10\n",
      "iteration 19700 / 20000: loss 1.000776\n",
      "iteration 19800 / 20000: loss 1.052190\n",
      "iteration 19900 / 20000: loss 0.835434\n",
      "iteration 20000 / 20000: loss 0.854222\n"
     ]
    }
   ],
   "source": [
    "from neural_net import *\n",
    "\n",
    "input_size = trainXC.shape[1]\n",
    "hidden_size = 150\n",
    "num_classes = 10\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes,1e-4)\n",
    "stats = net.train(trainXC, y_train, valXC, y_val,\n",
    "                            num_iters=20000, batch_size=100,\n",
    "                            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "                            reg=0, verbose=True,update=\"momentum\",arg=0.9,dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.686632653061\n",
      "Validation accuracy:  0.665\n",
      "Test accuracy:  0.678\n"
     ]
    }
   ],
   "source": [
    "val_acc = (net.predict(trainXC) == y_train).mean()\n",
    "print 'Train accuracy: ', val_acc\n",
    "val_acc = (net.predict(valXC) == y_val).mean()\n",
    "print 'Validation accuracy: ', val_acc\n",
    "\n",
    "val_acc = (net.predict(testXC) == y_test).mean()\n",
    "print 'Test accuracy: ', val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f7c8c6a32d0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the loss function and train / validation accuracies\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "#plt.savefig(\"dropout loss_history.eps\")\n",
    "\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "plt.ylabel('Clasification accuracy')\n",
    "#plt.savefig('dropout accuracy.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
